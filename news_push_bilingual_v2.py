#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
v2 æ¯æ—¥æ¨é€ï¼šå…¨çƒçƒ­ç‚¹(è‡ªåŠ¨ç¿»è¯‘) + å›½å†…çƒ­ç‚¹ åˆ°å¾®ä¿¡ï¼ˆServeré…±ï¼‰
------------------------------------------------------------
æ”¹åŠ¨è¦ç‚¹ï¼š
- ä½¿ç”¨ feedparser ä¼˜å…ˆè§£æ RSSï¼ˆå…¼å®¹æ€§æ›´å¼ºï¼‰ï¼Œå¤±è´¥å†é€€å›åˆ° BeautifulSoupã€‚
- æ›´æ–°æ›´ç¨³å®šçš„æ–°é—»æºï¼ˆBBC/CNN/Reutersï¼›æ–°åç¤¾/å¤®è§†/æ¾æ¹ƒï¼‰ã€‚
- åŠ å…¥è¯·æ±‚é‡è¯•ã€è¶…æ—¶ã€UA æ ‡å¤´ä¸ç®€å•é™æµï¼Œå‡å°‘å¶å‘å¤±è´¥ã€‚
- ç¿»è¯‘é“¾ï¼šOpenAI -> DeepL -> MyMemoryï¼ˆå‡å¯é€‰ï¼›æ— å¯†é’¥ä¹Ÿèƒ½è·‘ï¼‰ã€‚
- è¾“å‡ºä¸¥æ ¼ Markdownï¼Œé¿å…è¢«å¾®ä¿¡æŠ˜å ï¼›é“¾æ¥æ˜¾ç¤ºåŸŸåã€‚
è¿è¡Œï¼š
  pip install requests beautifulsoup4 feedparser
  export SERVERCHAN_SENDKEY=ä½ çš„Key
  python news_push_bilingual_v2.py
"""

import os
import time
import json
import html
import requests
import feedparser
from urllib.parse import urlparse, quote
from datetime import datetime, timezone
from bs4 import BeautifulSoup

TITLE = "ä»Šæ—¥çƒ­ç‚¹ç®€æŠ¥ï½œå…¨çƒ + å›½å†…"
TOP_K_PER_SOURCE = int(os.getenv("TOP_K_PER_SOURCE", "6"))

# ==== æ–°é—»æºï¼ˆå¯è‡ªè¡Œå¢åˆ ï¼‰====
GLOBAL_RSS = [
    "http://feeds.bbci.co.uk/news/world/rss.xml",       # BBC World
    "http://rss.cnn.com/rss/edition_world.rss",         # CNN World
    "https://feeds.reuters.com/reuters/worldNews",      # Reuters World
]
CHINA_RSS = [
    "http://www.news.cn/rss/politics.xml",              # æ–°åç¤¾ æ—¶æ”¿
    "https://news.cctv.com/data/rss/newsChina.xml",     # å¤®è§† å›½å†…
    "https://www.thepaper.cn/rss.jsp?nodeid=25434",     # æ¾æ¹ƒ å›½å†…è¦é—»
]

UA = os.getenv("HTTP_UA", "Mozilla/5.0 (NewsPushBot/2.0; +https://github.com/)")
TIMEOUT = 20
RETRIES = 2
SLEEP_BETWEEN = 0.6  # æ¯ä¸ªè¯·æ±‚ä¹‹é—´çš„é—´éš”ï¼Œé¿å…è§¦å‘é™æµ

session = requests.Session()
adapter = requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20, max_retries=0)
session.mount("http://", adapter)
session.mount("https://", adapter)
session.headers.update({"User-Agent": UA})

def host_of(link: str) -> str:
    try:
        return urlparse(link).netloc or "source"
    except Exception:
        return "source"

def get_text(obj, default=""):
    try:
        return (obj or "").strip()
    except Exception:
        return default

def fetch_via_feedparser(url, topk):
    try:
        d = feedparser.parse(url)
        items = []
        for e in d.entries[:topk]:
            title = get_text(e.get("title", ""))
            link = get_text(e.get("link", ""))
            pub = get_text(e.get("published", "")) or get_text(e.get("updated", ""))
            items.append((title, link, pub, host_of(link)))
        return items
    except Exception:
        return []

def fetch_via_bs4(url, topk):
    try:
        r = session.get(url, timeout=TIMEOUT)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "xml")
        items = []
        for item in soup.find_all("item")[:topk]:
            title = get_text(item.title.text if item.title else "")
            link = get_text(item.link.text if item.link else "")
            pub = get_text(item.pubDate.text if item.pubDate else "")
            items.append((title, link, pub, host_of(link)))
        if not items:
            for entry in soup.find_all("entry")[:topk]:
                title = get_text(entry.title.text if entry.title else "")
                link_tag = entry.find("link")
                link = get_text(link_tag.get("href") if link_tag else "")
                pub = get_text(entry.updated.text if entry.find("updated") else "")
                items.append((title, link, pub, host_of(link)))
        return items
    except Exception:
        return []

def fetch_rss_items(url, topk):
    for i in range(RETRIES + 1):
        items = fetch_via_feedparser(url, topk)
        if not items:
            items = fetch_via_bs4(url, topk)
        if items:
            return items
        time.sleep(1.2 + 0.3 * i)
    return [("ã€æŠ“å–å¤±è´¥ã€‘%s" % url, "", "", "error")]

def dedup(items):
    seen = set()
    out = []
    for t, l, p, h in items:
        key = (t.strip(), h)
        if key in seen or not t.strip():
            continue
        seen.add(key)
        out.append((t, l, p, h))
    return out

# ===== ç¿»è¯‘æ¨¡å— =====
def translate_openai(texts):
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return None
    url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1/chat/completions")
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    prompt = "å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»æ ‡é¢˜é€æ¡ç¿»è¯‘æˆç®€æ´çš„ä¸­æ–‡ï¼ˆä¿ç•™ä¸“æœ‰åè¯ï¼‰ï¼Œåªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š\n" + json.dumps(texts, ensure_ascii=False)
    payload = {
        "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.2,
    }
    try:
        resp = session.post(url, headers=headers, json=payload, timeout=40)
        resp.raise_for_status()
        content = resp.json()["choices"][0]["message"]["content"]
        arr = json.loads(content)
        if isinstance(arr, list) and len(arr) == len(texts):
            return [str(x).strip() for x in arr]
        return None
    except Exception:
        return None

def translate_deepl(texts):
    key = os.getenv("DEEPL_API_KEY")
    if not key:
        return None
    url = "https://api-free.deepl.com/v2/translate"
    outs = []
    try:
        for t in texts:
            data = {"auth_key": key, "text": t, "target_lang": "ZH"}
            r = session.post(url, data=data, timeout=20)
            r.raise_for_status()
            outs.append(r.json()["translations"][0]["text"])
            time.sleep(0.35)
        return outs if len(outs) == len(texts) else None
    except Exception:
        return None

def translate_mymemory(texts):
    outs = []
    for t in texts:
        try:
            url = f"https://api.mymemory.translated.net/get?q={quote(t)}&langpair=en|zh-CN"
            r = session.get(url, timeout=20)
            r.raise_for_status()
            data = r.json()
            outs.append(data.get("responseData", {}).get("translatedText", t))
            time.sleep(0.5)
        except Exception:
            outs.append(t)
    return outs

def auto_translate(texts):
    if not texts:
        return []
    def looks_chinese(s):
        for ch in s[:8]:
            if "\u4e00" <= ch <= "\u9fff":
                return True
        return False
    if all(looks_chinese(t) for t in texts):
        return texts
    outs = translate_openai(texts)
    if outs:
        return outs
    outs = translate_deepl(texts)
    if outs:
        return outs
    return translate_mymemory(texts)

# ===== å‘é€ =====
def send_serverchan(title, markdown):
    send_key = os.getenv("SERVERCHAN_SENDKEY")
    if not send_key:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ SERVERCHAN_SENDKEY")
    url = f"https://sctapi.ftqq.com/{send_key}.send"
    data = {"title": title, "desp": markdown}
    r = session.post(url, data=data, timeout=25)
    r.raise_for_status()
    return r.text

# ===== æ¸²æŸ“ =====
def build_markdown(globals_items, china_items, translated):
    now = datetime.now(timezone.utc).astimezone().strftime("%Y-%m-%d %H:%M")
    lines = []
    lines.append(f"**{TITLE}**  \næ›´æ–°ï¼š{now}\n")

    lines.append("### ğŸŒ å…¨çƒçƒ­ç‚¹ï¼ˆå·²è¯‘ï¼‰")
    for i, (item, zh) in enumerate(zip(globals_items, translated), 1):
        t, link, _, h = item
        t = html.escape(t)
        zh = html.escape(zh)
        if link:
            lines.append(f"{i}. {zh}  \n    *{t}*  \n    [{h}]({link})")
        else:
            lines.append(f"{i}. {zh}  \n    *{t}*")
    lines.append("")

    lines.append("### ğŸ‡¨ğŸ‡³ å›½å†…çƒ­ç‚¹")
    for i, (t, link, _, h) in enumerate(china_items, 1):
        t = html.escape(t)
        if link:
            lines.append(f"{i}. {t}  \n    [{h}]({link})")
        else:
            lines.append(f"{i}. {t}")
    return "\n".join(lines)

def main():
    g_items = []
    for src in GLOBAL_RSS:
        g_items += fetch_rss_items(src, TOP_K_PER_SOURCE)
        time.sleep(SLEEP_BETWEEN)
    g_items = dedup(g_items)

    c_items = []
    for src in CHINA_RSS:
        c_items += fetch_rss_items(src, TOP_K_PER_SOURCE)
        time.sleep(SLEEP_BETWEEN)
    c_items = dedup(c_items)

    g_titles = [x[0] for x in g_items]
    translated = auto_translate(g_titles)
    if len(translated) != len(g_items):
        translated = g_titles

    md = build_markdown(g_items, c_items, translated)
    resp = send_serverchan(TITLE, md)
    print(resp)

if __name__ == "__main__":
    main()
